This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-03-08T19:13:27.579Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
LICENSE
MANIFEST.in
project.md
pyproject.toml
pytest.ini
README.md
src/lfind/__init__.py
src/lfind/config.json
src/lfind/config.py
src/lfind/db_manager.py
src/lfind/embed_manager.py
src/lfind/embedding/__init__.py
src/lfind/embedding/base.py
src/lfind/embedding/file_embedders/__init__.py
src/lfind/embedding/file_embedders/pdf_embedder.py
src/lfind/embedding/file_embedders/text_embedder.py
src/lfind/embedding/models/__init__.py
src/lfind/embedding/models/embedding_model.py
src/lfind/embedding/models/openai_embeddings.py
src/lfind/embedding/models/sentence_transformers.py
src/lfind/embedding/registry.py
src/lfind/embedding/service.py
src/lfind/faiss_utils.py
src/lfind/index_manager.py
src/lfind/llm_service.py
src/lfind/main.py
src/lfind/requirements.txt
src/lfind/search_pipeline.py
summary.md
tests/run_all_tests.py
tests/test_basic.py
tests/test_embeddings.py
tests/test_index_manager.py
tests/test_search_pipeline.py

================================================================
Repository Files
================================================================

================
File: .gitignore
================
*pycache*
*.env
*.egg-info
dist
*.lfind_cache

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Samuel Buban

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

================
File: MANIFEST.in
================
include LICENSE
include README.md
include pyproject.toml
include src/lfind/config.json

================
File: project.md
================
# **Project Overview and Roadmap**

## **Project Overview**

**Project Name:**  
*LFind* – A Natural Language File Finder with Multi-Stage Filtering and Semantic Search

**Goal:**  
Develop a robust file search tool that empowers users to locate files (and directories) using both traditional metadata filters and advanced semantic methods. The tool combines:

- **Traditional Filtering:**  
  Using metadata (absolute paths, file extensions, dates, sizes, etc.) stored in a lightweight SQLite database.

- **Vector-Based Similarity Search:**  
  Utilizing FAISS to perform rapid semantic searches over file embeddings (derived from file titles, content, or summaries).

- **LLM-Assisted Search:**  
  Employing large language models (LLMs) to interpret natural language queries and map them to relevant file metadata and content. This semantic search can also operate on the filtered candidate set.

Users can first perform a basic search by specifying filters (such as directory, extension, date, and file size), then refine the results using similarity search techniques. By centralizing metadata in the SQLite database, the tool remains simple to install and maintain while scaling to millions of entries.

---

## **Roadmap**

1. **Data Acquisition & Metadata Extraction**
   - **File Scanning:**  
     Traverse the file system to read all files and directories.  
     *Note:* The separate tree structure cache is no longer needed because all hierarchy details will be stored in the database.
   - **Metadata Population:**  
     For each file/directory, extract and compute:
     - Absolute path (and optionally a dedicated directory field)
     - File attributes (extension, creation/modification dates, file size, type)
     - Unique identifier (int64 ID)
   - **Database Storage:**  
     Insert these metadata records into an SQLite database. Ensure proper indexing (on the absolute path, directory, dates, etc.) for fast query performance.

2. **Vector Embedding & FAISS Indexing**
   - **Embedding Generation:**  
     Compute embeddings (for file titles, content summaries, etc.) using your chosen model.
   - **FAISS Index Construction:**  
     Create a FAISS index (wrapped with `IndexIDMap`) to store each embedding along with its unique ID from the metadata database.
   - **Storage & Synchronization:**  
     The FAISS index and SQLite database remain synchronized by using the unique ID as the common key.

3. **Multi-Stage Query Pipeline**
   
   **Stage 1: Basic Metadata Filtering**
   - **User Input:**  
     Accept filters from the user (e.g., search within a specific directory, by file extension, date range, file size).
   - **SQLite Query:**  
     Run SQL queries on the metadata database to retrieve the candidate file IDs that match the criteria.
   - **Output:**  
     Obtain a subset of file IDs that represent the filtered set of files.

   **Stage 2: Semantic Similarity Search**
   - **Approach Options:**
     - **Re-indexing Candidate Embeddings:**  
       Retrieve embeddings for the candidate IDs and build a temporary FAISS index to run a similarity search on a limited set.
     - **Post-Filtering Full Index Results:**  
       Run the similarity search on the full FAISS index and then filter the results by intersecting with the candidate IDs.
   - **User-Extended Search:**  
     Allow the user to refine the initial metadata-based results with a semantic query—either via vector similarity or LLM-based search—ensuring that the search operates only on the filtered subset.

4. **LLM Integration**
   - **Natural Language Search:**  
     Integrate the existing LLM-based search capability. Construct prompts using the candidate file summaries or names from the metadata database.
   - **Filtered LLM Search:**  
     Ensure that the LLM search uses the same candidate set from the metadata filtering stage to return contextually relevant results.

5. **User Interface (Console/CLI)**
   - **Input & Interaction:**  
     Develop a simple console UI that:
     - Prompts the user for filtering criteria (directory, extensions, dates, sizes).
     - Displays results from the basic metadata query.
     - Accepts a follow-up semantic query for further refinement.
   - **Output:**  
     Show the final list of file paths (and relevant details) corresponding to the matching files.

6. **Testing, Optimization, and Documentation**
   - **Testing:**  
     Write unit and integration tests to validate metadata extraction, database queries, FAISS similarity search, and LLM prompt generation.
   - **Performance Benchmarking:**  
     Compare the performance of re-indexing the candidate set versus post-filtering the full FAISS search results. Tune the SQLite queries and FAISS parameters based on empirical tests.
   - **Documentation:**  
     Update the project README and user guides to detail installation (using SQLite and FAISS, both of which require no separate servers), configuration options, and usage instructions.

7. **Final Integration and Deployment**
   - **Configuration:**  
     Provide a configuration file that allows users to customize filtering options, database paths, and index settings.
   - **Packaging:**  
     Package the project as a standalone Python application or module that can be easily installed via pip.
   - **Deployment:**  
     Ensure that the final product integrates all components seamlessly and provides clear usage instructions.

---

# **Final Project Vision**

*LFind* will be a single, self-contained file search tool that combines the speed of vector similarity search (via FAISS) with the precision of metadata filtering (via SQLite). Users will be able to:
- **Filter files:** Quickly narrow down their search using criteria such as directory, file extension, date, and size.
- **Refine results:** Perform semantic similarity searches on the filtered candidate set—either through vector similarity or LLM-based methods.
- **Seamless Integration:** Enjoy a simple installation process with no need for external servers (thanks to SQLite) and benefit from fast, efficient queries even on large file repositories.
- **User-Friendly Interface:** Interact with the tool via a straightforward console UI that guides them through both basic and advanced search options.

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "lfind"
version = "0.2.0"
description = "A natural language file finder using LLMs"
authors = [
    {name = "Samuel Buban", email = "samuelbuban@gmail.com"}
]
license = {file = "LICENSE"}
readme = "README.md"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "openai>=1.61.1",
    "python-dotenv>=1.0.1",
    "faiss-cpu>=1.7.0",
    "sentence-transformers>=2.2.0",
    "PyPDF2>=3.0.0",  # For PDF text extraction
]

[project.urls]
Homepage = "https://github.com/Mahrkeenerh/lfind"
Repository = "https://github.com/Mahrkeenerh/lfind.git"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
include = ["lfind*"]

[project.scripts]
lfind = "lfind.main:main"

================
File: pytest.ini
================
[pytest]
addopts = -v --tb=short
testpaths = tests
log_cli_level = INFO
log_file_level = DEBUG
python_files = test_*.py
filterwarnings = ignore::DeprecationWarning
markers =
    database: mark test as a database test
    functionality: mark test as a functionality test
    performance: mark test as a performance test

================
File: README.md
================
# lfind

A natural language file finder using LLMs. This tool allows you to search for files using natural language queries, powered by large language models. Simply describe what you're looking for, and lfind will help you locate the relevant files.

### Privacy Warning

If you are using OpenAI models (or any other 3rd party models), be aware of the potential privacy implications. The model provider may have access to your queries and the files you are searching for. If you are concerned about privacy, consider using local models like Ollama or other privacy-preserving alternatives only.

## Installation

```bash
pip install lfind
```

## Configuration

lfind uses a configuration file to manage its settings. The default configuration is installed with the package, and a user-specific configuration is automatically created on first run at:

- Windows: `%APPDATA%\lfind\config.json`
- Unix/Linux: `~/.config/lfind/config.json`

### Default Configuration

```json
{
    "max_entries": 100,
    "ignore_patterns": [
        ".*"
    ],
    "include_empty_dirs": false,
    "cache_dir": ".lfind_cache",
    "meta_cache": "meta_cache.json",
    "cache_validity_days": 7,
    "llm_default": {
        "provider": "ollama",
        "model": "qwen2.5:14b-instruct-q6_K",
        "api_base": "http://localhost:11434/v1"
    },
    "llm_hard": {
        "provider": "openai",
        "model": "gpt-4o",
        "api_base": null
    }
}
```

### Environment Variables

If you're using OpenAI models (default for the "hard" mode), you need to set your OpenAI API key:

```bash
export OPENAI_API_KEY=your_api_key_here  # Unix/Linux
setx OPENAI_API_KEY your_api_key_here     # Windows
```

## Usage

```bash
# Basic search in current directory
lfind "First invoice of 2025"

# Search with specific file extensions
lfind "presentation on attention mechanisms" -e pdf docx

# Search in specific directory
lfind "log files from last week" -d /path/to/dir

# Use more powerful LLM model (e.g., GPT-4o)
lfind "files related to database migrations" -H

# Ignore default ignore patterns (ommiting . (dot) files)
lfind "configuration files" --ignore-defaults

# Add custom ignore patterns
lfind "important documents" -i "*.tmp" "*.bak"
```

### Additional Options

- `--empty keep/ignore`: Override empty directories behavior
- `--max N`: Set maximum entries per directory
- `-r, --refresh-cache`: Force rebuild the cache
- `--config`: Set custom configuration file
- `-v, --version`: Show version information

## Caching

lfind maintains a cache of the directory structure to improve performance. The cache is automatically created and updated as needed, with a default validity period of 7 days. You can force a cache refresh using the `-r` flag.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Requirements

- Python ≥ 3.8
- openai ≥ 1.61.1
- python-dotenv ≥ 1.0.1

## Note

When using OpenAI models (default for hard mode), ensure you have set your OPENAI_API_KEY environment variable. For local models like Ollama, ensure the service is running and accessible at the configured API base URL.

================
File: src/lfind/__init__.py
================
"""lfind - Natural language file finder using LLMs."""
from . import main

================
File: src/lfind/config.json
================
{
    "max_entries": 100,
    "ignore_patterns": [
        ".*"
    ],
    "include_empty_dirs": false,
    "cache_dir": ".lfind_cache",
    "meta_cache": "meta_cache.json",
    "cache_validity_days": 7,
    "llm_default": {
        "provider": "ollama",
        "model": "qwen2.5:14b-instruct-q6_K",
        "api_base": "http://localhost:11434/v1"
    },
    "llm_hard": {
        "provider": "openai",
        "model": "gpt-4o",
        "api_base": null
    }
}

================
File: src/lfind/config.py
================
import json
import os
from pathlib import Path
import shutil
import sys


def get_default_config_path():
    """Get the default config file path from the package."""
    import lfind
    package_dir = os.path.dirname(lfind.__file__)
    return os.path.join(package_dir, "config.json")


def get_user_config_path():
    """Get the user's config file path."""
    if sys.platform == "win32":
        config_dir = os.path.join(os.environ["APPDATA"], "lfind")
    else:
        config_dir = os.path.join(str(Path.home()), ".config", "lfind")

    return os.path.join(config_dir, "config.json")


def ensure_user_config():
    """Ensure user config exists, create if it doesn't."""
    user_config_path = get_user_config_path()
    if not os.path.exists(user_config_path):
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(user_config_path), exist_ok=True)

        # Copy default config to user config location
        default_config_path = get_default_config_path()
        shutil.copy2(default_config_path, user_config_path)

    return user_config_path


def load_config(config_path=None):
    """
    Load configuration from a JSON file; return a dictionary of settings.
    If no config_path is provided, use the user config or create it from default.
    """
    defaults = {
        "max_entries": 100,
        "ignore_patterns": [".*"],  # Default pattern to ignore all dot files/folders
        "include_empty_dirs": False,
        "cache_dir": ".lfind_cache",  # Directory to store all caches
        "meta_cache": "meta_cache.json",
        "cache_validity_days": 7
    }

    # If no specific path provided, use user config
    if config_path is None:
        config_path = ensure_user_config()

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            user_config = json.load(f)
        defaults.update(user_config)
    except Exception as e:
        print(f"Warning: Could not load config file '{config_path}': {e}\nUsing default settings.", 
              file=sys.stderr)

    return defaults


def get_config_location():
    """Get the location of the currently used config file."""
    return get_user_config_path()

================
File: src/lfind/db_manager.py
================
from datetime import datetime
import os
import sqlite3
from typing import Dict, List, Optional, Tuple, Union


class DatabaseManager:
    """Manages SQLite database operations for file metadata storage and retrieval."""
    
    def __init__(self, db_path: str = ".lfind/metadata.db"):
        """Initialize database connection and ensure tables exist."""
        self.db_path = db_path
        self._connection: Optional[sqlite3.Connection] = None

        # Ensure database directory exists
        os.makedirs(os.path.dirname(db_path), exist_ok=True)

        # Initialize database and create tables (including the seen flag)
        self._init_database()

    def _get_connection(self) -> sqlite3.Connection:
        """Get a database connection."""
        if self._connection is None:
            try:
                self._connection = sqlite3.connect(self.db_path)
                self._connection.row_factory = sqlite3.Row
            except sqlite3.Error as e:
                print(f"Error connecting to database: {e}")
                raise
        return self._connection

    def _init_database(self):
        """Initialize database schema."""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()

            # Create files table with a 'seen' flag for scheduled updates.
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    absolute_path TEXT NOT NULL UNIQUE,
                    type TEXT NOT NULL,
                    extension TEXT,
                    size INTEGER,
                    created_at TIMESTAMP,
                    modified_at TIMESTAMP,
                    last_indexed_at TIMESTAMP,
                    embedding_id INTEGER,
                    embedding_type TEXT,
                    seen INTEGER DEFAULT 0
                )
            """)

            # Create indexes for common query patterns.
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_path ON files(absolute_path)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_extension ON files(extension)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_type ON files(type)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_modified ON files(modified_at)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_embedding_type ON files(embedding_type)")
            conn.commit()
        except sqlite3.Error as e:
            print(f"Error initializing database: {e}")
            raise

    def close(self):
        """Close database connection."""
        if self._connection:
            self._connection.close()
            self._connection = None

    def reset_seen_flags(self):
        """Reset the seen flag for all records to 0."""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE files SET seen = 0")
            conn.commit()
        except sqlite3.Error as e:
            print(f"Error resetting seen flags: {e}")
            raise

    def delete_missing_files(self) -> int:
        """
        Delete all records where the seen flag is still 0,
        indicating that these files were not encountered during the scheduled update.
        Returns the number of deleted records.
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("DELETE FROM files WHERE seen = 0")
            count = cursor.rowcount
            conn.commit()
            return count
        except sqlite3.Error as e:
            print(f"Error deleting missing files: {e}")
            return 0

    def touch_file(self, file_data: Dict[str, Union[str, int, float]]) -> bool:
        """
        Evaluate and update the file record for the given file.
        This method:
          - Reads the current file stats (size, modified_at, etc.) from disk.
          - If the record does not exist, inserts a new record with seen = 1 and returns True.
          - If the record exists but is outdated (e.g., modified_at or size differ), updates the record,
            sets seen = 1, and returns True.
          - Otherwise, simply marks the record as seen (sets seen = 1) and returns False.
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()

            # Get file stats
            try:
                stats = os.stat(file_data['absolute_path'])
                size = stats.st_size
                created_at = datetime.fromtimestamp(stats.st_ctime)
                modified_at = datetime.fromtimestamp(stats.st_mtime)
            except (OSError, FileNotFoundError) as e:
                print(f"File stat error for {file_data['absolute_path']}: {e}")
                return False  # If file cannot be read, skip update.

            # Get file extension
            _, extension = os.path.splitext(file_data['name'])
            extension = extension.lower() if extension else None

            # Try to find an existing record.
            cursor.execute("SELECT * FROM files WHERE absolute_path = ?", (file_data['absolute_path'],))
            row = cursor.fetchone()
            requires_update = False

            if row is None:
                # No record exists: insert new record and mark as seen.
                cursor.execute("""
                    INSERT INTO files (
                        name, absolute_path, type, extension,
                        size, created_at, modified_at, last_indexed_at, seen
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, 1)
                """, (
                    file_data['name'],
                    file_data['absolute_path'],
                    file_data['type'],
                    extension,
                    size,
                    created_at,
                    modified_at
                ))
                requires_update = True
            else:
                # Record exists. Compare stored modified_at and size.
                old_modified = row['modified_at']
                old_size = row['size']

                try:
                    old_modified_dt = datetime.fromisoformat(old_modified) if old_modified else None
                except Exception:
                    old_modified_dt = None

                if (old_modified_dt is None or modified_at != old_modified_dt) or (old_size is None or size != old_size):
                    # File has changed: update record.
                    requires_update = True
                    cursor.execute("""
                        UPDATE files SET
                            name = ?,
                            type = ?,
                            extension = ?,
                            size = ?,
                            created_at = ?,
                            modified_at = ?,
                            last_indexed_at = CURRENT_TIMESTAMP,
                            seen = 1
                        WHERE absolute_path = ?
                    """, (
                        file_data['name'],
                        file_data['type'],
                        extension,
                        size,
                        created_at,
                        modified_at,
                        file_data['absolute_path']
                    ))
                else:
                    # File is up-to-date: simply mark it as seen.
                    cursor.execute("UPDATE files SET seen = 1 WHERE absolute_path = ?", (file_data['absolute_path'],))
            conn.commit()
            return requires_update
        except sqlite3.Error as e:
            print(f"Error in touch_file for {file_data['absolute_path']}: {e}")
            return False

    def get_files_by_criteria(
        self,
        directory: Optional[str] = None,
        extensions: Optional[List[str]] = None,
        file_type: Optional[str] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        modified_after: Optional[datetime] = None,
        modified_before: Optional[datetime] = None
    ) -> List[Dict[str, Union[str, int, float]]]:
        """
        Retrieve files matching the specified criteria.
        Returns a list of file metadata dictionaries.
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            query = "SELECT * FROM files WHERE 1=1"
            params = []

            if directory:
                # Recursive filtering: files whose absolute path starts with the given directory.
                query += " AND absolute_path LIKE ?"
                params.append(directory.rstrip('/') + '/%')

            if extensions:
                query += " AND extension IN (" + ','.join('?' for _ in extensions) + ")"
                params.extend(extensions)

            if file_type:
                query += " AND type = ?"
                params.append(file_type)

            if min_size is not None:
                query += " AND size >= ?"
                params.append(min_size)

            if max_size is not None:
                query += " AND size <= ?"
                params.append(max_size)

            if modified_after:
                query += " AND modified_at >= ?"
                params.append(modified_after)

            if modified_before:
                query += " AND modified_at <= ?"
                params.append(modified_before)

            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
        except sqlite3.Error as e:
            print(f"Error in get_files_by_criteria: {e}")
            return []

    def get_embedding_mappings(self) -> List[Tuple[int, int, str]]:
        """Get all file ID to embedding ID mappings with embedding type."""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, embedding_id, embedding_type
                FROM files
                WHERE embedding_id IS NOT NULL
            """)
            return [(row['id'], row['embedding_id'], row['embedding_type']) for row in cursor.fetchall()]
        except sqlite3.Error as e:
            print(f"Error in get_embedding_mappings: {e}")
            return []

    def get_file_by_id(self, file_id: int) -> Optional[Dict[str, Union[str, int, float]]]:
        """Retrieve file metadata by ID."""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM files WHERE id = ?", (file_id,))
            row = cursor.fetchone()
            return dict(row) if row else None
        except sqlite3.Error as e:
            print(f"Error in get_file_by_id: {e}")
            return None

    def get_files_by_embedding_ids(self, embedding_ids: List[int], embedding_type: Optional[str] = None) -> List[Dict[str, Union[str, int, float]]]:
        """
        Retrieve file metadata for files with specified embedding IDs.
        Optionally filter by embedding_type ('title' or 'content').
        Returns results in an arbitrary order.
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            placeholders = ','.join('?' for _ in embedding_ids)
            query = f"SELECT * FROM files WHERE embedding_id IN ({placeholders})"
            params = list(embedding_ids)

            if embedding_type:
                query += " AND embedding_type = ?"
                params.append(embedding_type)

            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
        except sqlite3.Error as e:
            print(f"Error in get_files_by_embedding_ids: {e}")
            return []

    def update_embedding_id(self, file_id: int, embedding_id: int, embedding_type: str = 'title') -> bool:
        """
        Update the embedding ID and type for a specific file.

        Args:
            file_id: The ID of the file record
            embedding_id: The new embedding ID to associate with this file
            embedding_type: The type of embedding ('title' or 'content')

        Returns:
            True if successful, False otherwise
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE files 
                SET embedding_id = ?, embedding_type = ?
                WHERE id = ?
            """, (embedding_id, embedding_type, file_id))
            conn.commit()
            return cursor.rowcount > 0
        except sqlite3.Error as e:
            print(f"Error updating embedding ID: {e}")
            return False
    
    def get_files_by_embedding_type(self, embedding_type: str) -> List[Dict[str, Union[str, int, float]]]:
        """
        Retrieve all files with a specific embedding type.

        Args:
            embedding_type: The type of embedding ('title' or 'content')

        Returns:
            List of file metadata dictionaries
        """
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM files WHERE embedding_type = ?", (embedding_type,))
            return [dict(row) for row in cursor.fetchall()]
        except sqlite3.Error as e:
            print(f"Error in get_files_by_embedding_type: {e}")
            return []




### **Usage in a Scheduled Update**

# - **Step 1:** Call `reset_seen_flags()` to mark all records as unseen.
# - **Step 2:** Walk through your file system (using `os.walk` or similar). For each file/directory, prepare a `file_data` dictionary (including at least `'name'`, `'absolute_path'`, and `'type'`) and call `touch_file(file_data)`.  
#   - If `touch_file` returns `True`, you know this file is new or has been modified (and you can trigger any extra update actions, such as re-computing embeddings).
# - **Step 3:** After processing all files, call `delete_missing_files()` to remove records that were not "touched" (i.e. files that no longer exist on disk).

# This design allows you to update your metadata database in a single pass through the file system and then remove stale records in one final cleanup step.

================
File: src/lfind/embed_manager.py
================
import faiss
import numpy as np
import os

class EmbedManager:
    """
    A manager for handling FAISS indexing operations.
    This class creates and handles a FAISS index, allowing insertion, searching,
    and persistence of embeddings.

    Note: Deletion is not supported by default in FAISS's flat-index structures.
    """

    def __init__(self, dim: int, metric: str = 'ip'):
        """
        Initialize the EmbedManager with a specified embedding dimension and a similarity metric.
        Supported metrics: 'l2', 'ip' (dot product), 'cosine'.

        Args:
            dim (int): Dimension of the embeddings.
            metric (str): Similarity metric.
        """
        self.dim = dim
        self.metric = metric.lower()
        if self.metric == 'l2':
            self.index = faiss.IndexFlatL2(dim)
        elif self.metric in ['ip', 'cosine']:
            self.index = faiss.IndexFlatIP(dim)
        else:
            raise ValueError(f"Unsupported metric: {metric}")

    def add_embeddings(self, embeddings, ids=None):
        """
        Add embeddings to the FAISS index.
        For cosine similarity, embeddings are first normalized.

        Args:
            embeddings (array-like): A 2D array of shape (n, dim).
            ids: Not used with these indices.
        """
        embeddings = np.array(embeddings, dtype='float32')
        if embeddings.ndim != 2 or embeddings.shape[1] != self.dim:
            raise ValueError(f"Embeddings must be a 2D array with shape (n, {self.dim})")
        if self.metric == 'cosine':
            norm = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / (norm + 1e-10)
        self.index.add(embeddings)

    def search(self, query_embedding, k=10):
        """
        Search for nearest neighbors.
        For cosine similarity, normalize the query embedding.

        Args:
            query_embedding (array-like): 1D or 2D array.
            k (int): Number of nearest neighbors.

        Returns:
            distances, indices: Results from FAISS search.
        """
        query_embedding = np.array(query_embedding, dtype='float32')
        if query_embedding.ndim == 1:
            query_embedding = query_embedding.reshape(1, -1)
        if query_embedding.shape[1] != self.dim:
            raise ValueError(f"Query embedding must have dimension {self.dim}")
        if self.metric == 'cosine':
            norm = np.linalg.norm(query_embedding, axis=1, keepdims=True)
            query_embedding = query_embedding / (norm + 1e-10)
        distances, indices = self.index.search(query_embedding, k)
        return distances, indices

    def save_index(self, path: str):
        """
        Save the current FAISS index to disk.

        Args:
            path (str): The file path where the index will be saved.
        """
        directory = os.path.dirname(path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)
        faiss.write_index(self.index, path)

    def load_index(self, path: str):
        """
        Load a FAISS index from disk.

        Args:
            path (str): The file path of the saved index.
        
        Raises:
            FileNotFoundError: If the specified index file does not exist.
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Index file '{path}' does not exist.")
        self.index = faiss.read_index(path)

    def total_embeddings(self) -> int:
        """
        Returns the total number of embeddings stored in the index.

        Returns:
            int: Number of embeddings in the index.
        """
        return self.index.ntotal

================
File: src/lfind/embedding/__init__.py
================
from .base import FileEmbedder
from .registry import registry
from .service import EmbeddingService

# Import file embedders to ensure they're registered
from .file_embedders import TextFileEmbedder, PDFEmbedder

__all__ = [
    'FileEmbedder',
    'registry',
    'EmbeddingService',
    'TextFileEmbedder',
    'PDFEmbedder',
]

================
File: src/lfind/embedding/base.py
================
from abc import ABC, abstractmethod
import os
from typing import Dict, List, Any, Set, Optional
import numpy as np

class FileEmbedder(ABC):
    """Base class for all file embedders."""
    
    @property
    @abstractmethod
    def supported_extensions(self) -> Set[str]:
        """Return a set of file extensions this embedder can handle."""
        pass
    
    def can_embed(self, file_path: str) -> bool:
        """Check if this embedder can handle the given file."""
        ext = os.path.splitext(file_path)[1].lower()
        return ext in self.supported_extensions
    
    @abstractmethod
    def extract_text(self, file_path: str) -> str:
        """Extract text content from the file for embedding.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Extracted text content
        """
        pass
    
    @abstractmethod
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract metadata from the file that might be useful for search.
        
        This could include things like:
        - Title
        - Description/Summary
        - Keywords
        - Author
        - Created/Modified dates
        
        Args:
            file_path: Path to the file
            
        Returns:
            Dict[str, Any]: Metadata extracted from the file
        """
        pass

================
File: src/lfind/embedding/file_embedders/__init__.py
================
from ..registry import registry
from .text_embedder import TextFileEmbedder
from .pdf_embedder import PDFEmbedder

__all__ = [
    'TextFileEmbedder',
    'PDFEmbedder',
]

# Register the default embedders
registry.register(TextFileEmbedder())
registry.register(PDFEmbedder())

================
File: src/lfind/embedding/file_embedders/pdf_embedder.py
================
import os
import datetime
from typing import Dict, Any, Set

from ..base import FileEmbedder

class PDFEmbedder(FileEmbedder):
    """Embedder for PDF files."""
    
    @property
    def supported_extensions(self) -> Set[str]:
        return {'.pdf'}
    
    def extract_text(self, file_path: str, max_pages: int = 5) -> str:
        """Extract text from a PDF file.
        
        Args:
            file_path: Path to the PDF file
            max_pages: Maximum number of pages to extract
            
        Returns:
            Extracted text content
        """
        try:
            # Dynamically import PyPDF2
            import PyPDF2
            
            text = ""
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                
                # Get number of pages
                num_pages = len(reader.pages)
                pages_to_read = min(max_pages, num_pages)
                
                # Extract text from pages
                for i in range(pages_to_read):
                    page = reader.pages[i]
                    text += page.extract_text() + "\n\n"
                
                # Add a note if we truncated
                if max_pages < num_pages:
                    text += f"[Note: Only the first {max_pages} pages of {num_pages} were processed]"
                    
            return text
            
        except ImportError:
            return f"[Error: PyPDF2 library not installed. Cannot extract text from PDF.]"
        except Exception as e:
            print(f"Error extracting text from PDF {file_path}: {e}")
            return f"[Error extracting text: {str(e)}]"
    
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract metadata from a PDF file."""
        stats = os.stat(file_path)
        
        try:
            import PyPDF2
            
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                info = reader.metadata or {}
                
                # Extract metadata
                title = info.get('/Title', os.path.basename(file_path))
                author = info.get('/Author', 'Unknown')
                creator = info.get('/Creator', 'Unknown')
                producer = info.get('/Producer', 'Unknown')
                subject = info.get('/Subject', '')
                num_pages = len(reader.pages)
                
                # Get a summary from the first page
                summary = ""
                if num_pages > 0:
                    first_page_text = reader.pages[0].extract_text()
                    if first_page_text:
                        summary = first_page_text[:200] + "..." if len(first_page_text) > 200 else first_page_text
                
                return {
                    'filename': os.path.basename(file_path),
                    'title': title,
                    'author': author,
                    'creator': creator,
                    'producer': producer,
                    'subject': subject,
                    'pages': num_pages,
                    'size': stats.st_size,
                    'created': datetime.datetime.fromtimestamp(stats.st_ctime).isoformat(),
                    'modified': datetime.datetime.fromtimestamp(stats.st_mtime).isoformat(),
                    'summary': summary
                }
                
        except ImportError:
            # Fallback if PyPDF2 isn't installed
            return {
                'filename': os.path.basename(file_path),
                'size': stats.st_size,
                'created': datetime.datetime.fromtimestamp(stats.st_ctime).isoformat(),
                'modified': datetime.datetime.fromtimestamp(stats.st_mtime).isoformat(),
                'error': 'PyPDF2 library not installed'
            }
        except Exception as e:
            # Fallback if there's an error
            return {
                'filename': os.path.basename(file_path),
                'size': stats.st_size,
                'created': datetime.datetime.fromtimestamp(stats.st_ctime).isoformat(),
                'modified': datetime.datetime.fromtimestamp(stats.st_mtime).isoformat(),
                'error': str(e)
            }

================
File: src/lfind/embedding/file_embedders/text_embedder.py
================
import os
import datetime
from typing import Dict, Any, Set
import numpy as np

from ..base import FileEmbedder

class TextFileEmbedder(FileEmbedder):
    """Embedder for plain text files."""
    
    @property
    def supported_extensions(self) -> Set[str]:
        return {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.csv', '.yml', 
                '.yaml', '.ini', '.cfg', '.conf', '.sh', '.bat', '.ps1', '.sql', '.log'}
    
    def extract_text(self, file_path: str, max_length: int = 10000) -> str:
        """Extract text content from a text file.
        
        Args:
            file_path: Path to the text file
            max_length: Maximum number of characters to extract
            
        Returns:
            Extracted text content
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read(max_length)
            return content
        except UnicodeDecodeError:
            # Try with a different encoding
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read(max_length)
                return content
            except Exception as e:
                print(f"Error reading text file {file_path}: {e}")
                return f"[Error reading file: {str(e)}]"
        except Exception as e:
            print(f"Error reading text file {file_path}: {e}")
            return f"[Error reading file: {str(e)}]"
    
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract metadata from a text file."""
        stats = os.stat(file_path)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # Get the first few lines for a summary
                lines = []
                for i, line in enumerate(f):
                    if i >= 10:
                        break
                    line = line.strip()
                    if line:
                        lines.append(line)
                
                summary = '\n'.join(lines[:3]) if lines else ""
        except:
            summary = "[Error reading file summary]"
            
        return {
            'filename': os.path.basename(file_path),
            'size': stats.st_size,
            'created': datetime.datetime.fromtimestamp(stats.st_ctime).isoformat(),
            'modified': datetime.datetime.fromtimestamp(stats.st_mtime).isoformat(),
            'summary': summary
        }

================
File: src/lfind/embedding/models/__init__.py
================
from .embedding_model import EmbeddingModel
from .sentence_transformers import SentenceTransformerModel
from .openai_embeddings import OpenAIEmbeddingModel

__all__ = [
    'EmbeddingModel',
    'SentenceTransformerModel',
    'OpenAIEmbeddingModel',
]

def get_model(provider: str, model_name: str = None) -> EmbeddingModel:
    """Factory function to get the appropriate embedding model.
    
    Args:
        provider: Model provider ('openai', 'sentence-transformers')
        model_name: Specific model name
        
    Returns:
        An embedding model instance
    """
    provider = provider.lower()
    
    if provider == 'openai':
        model_name = model_name or "text-embedding-3-small"
        return OpenAIEmbeddingModel(model_name=model_name)
    elif provider in ('sentence-transformers', 'st', 'huggingface', 'hf'):
        model_name = model_name or "all-MiniLM-L6-v2"
        return SentenceTransformerModel(model_name=model_name)
    else:
        raise ValueError(f"Unsupported embedding provider: {provider}")

================
File: src/lfind/embedding/models/embedding_model.py
================
from abc import ABC, abstractmethod
from typing import List, Union
import numpy as np

class EmbeddingModel(ABC):
    """Base class for embedding models."""
    
    @abstractmethod
    def get_embedding_dimension(self) -> int:
        """Return the dimension of the embeddings produced by this model."""
        pass
    
    @abstractmethod
    def get_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            numpy array containing the embedding
        """
        pass
    
    @abstractmethod
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """Generate embeddings for multiple texts.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            2D numpy array with embeddings
        """
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """Return the name of this embedding model."""
        pass
    
    @abstractmethod
    def get_provider(self) -> str:
        """Return the provider of this embedding model."""
        pass

================
File: src/lfind/embedding/models/openai_embeddings.py
================
from typing import List
import numpy as np
import os
import time

from .embedding_model import EmbeddingModel

class OpenAIEmbeddingModel(EmbeddingModel):
    """Embedding model using OpenAI's API."""
    
    # OpenAI embedding models and their dimensions
    _MODEL_DIMENSIONS = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536
    }
    
    def __init__(self, model_name: str = "text-embedding-3-small"):
        """Initialize the model.
        
        Args:
            model_name: Name of the OpenAI embedding model to use
        """
        if model_name not in self._MODEL_DIMENSIONS:
            supported = ", ".join(self._MODEL_DIMENSIONS.keys())
            raise ValueError(
                f"Unsupported model: {model_name}. " 
                f"Supported models are: {supported}"
            )
            
        self.model_name = model_name
        self._dimension = self._MODEL_DIMENSIONS[model_name]
        
        try:
            import openai
            self.client = openai.OpenAI()
        except ImportError:
            raise ImportError(
                "OpenAI Python client is not installed. "
                "Install it with 'pip install openai'."
            )
    
    def get_embedding_dimension(self) -> int:
        return self._dimension
    
    def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for a single text using OpenAI API."""
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )
            embedding = response.data[0].embedding
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            print(f"Error getting OpenAI embedding: {e}")
            # Return a zero vector as fallback
            return np.zeros(self._dimension, dtype=np.float32)
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """Get embeddings for multiple texts using OpenAI API.
        
        Handles rate limiting and batching.
        """
        results = []
        batch_size = 100  # OpenAI recommends batches of ~100
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                batch_embeddings = [data.embedding for data in response.data]
                results.extend(batch_embeddings)
                
                # Sleep briefly to avoid rate limits if many batches
                if i + batch_size < len(texts):
                    time.sleep(0.5)
                    
            except Exception as e:
                print(f"Error batch embedding texts {i} to {i+len(batch)}: {e}")
                # Add zero vectors for the failed batch
                zeros = [np.zeros(self._dimension) for _ in range(len(batch))]
                results.extend(zeros)
        
        return np.array(results, dtype=np.float32)
    
    def get_name(self) -> str:
        return self.model_name
    
    def get_provider(self) -> str:
        return "OpenAI"

================
File: src/lfind/embedding/models/sentence_transformers.py
================
from typing import List, Optional
import numpy as np
import os

from .embedding_model import EmbeddingModel

class SentenceTransformerModel(EmbeddingModel):
    """Embedding model using SentenceTransformers library."""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """Initialize the model.
        
        Args:
            model_name: Name of the SentenceTransformers model to use
        """
        try:
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(model_name)
            self.model_name = model_name
            self._dimension = self.model.get_sentence_embedding_dimension()
        except ImportError:
            raise ImportError(
                "SentenceTransformers is not installed. "
                "Install it with 'pip install sentence-transformers'."
            )
    
    def get_embedding_dimension(self) -> int:
        return self._dimension
    
    def get_embedding(self, text: str) -> np.ndarray:
        return self.model.encode(text, show_progress_bar=False)
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        return self.model.encode(texts, show_progress_bar=True)
    
    def get_name(self) -> str:
        return self.model_name
    
    def get_provider(self) -> str:
        return "SentenceTransformers"

================
File: src/lfind/embedding/registry.py
================
import os
from typing import Dict, List, Type, Set, Optional
from .base import FileEmbedder

class EmbedderRegistry:
    """Registry for file embedders."""
    
    _instance = None
    
    def __new__(cls):
        """Singleton pattern to ensure we only have one registry."""
        if cls._instance is None:
            cls._instance = super(EmbedderRegistry, cls).__new__(cls)
            cls._instance._embedders = {}
            cls._instance._extension_map = {}
        return cls._instance
    
    def register(self, embedder: FileEmbedder, name: str = None) -> None:
        """Register a new embedder.
        
        Args:
            embedder: The embedder instance
            name: Optional name for the embedder. If None, class name will be used.
        """
        if name is None:
            name = embedder.__class__.__name__
        
        self._embedders[name] = embedder
        
        # Update extension mappings
        for ext in embedder.supported_extensions:
            if ext in self._extension_map:
                print(f"Warning: Extension {ext} already handled by "
                      f"{self._extension_map[ext]}, overriding with {name}")
            self._extension_map[ext] = name
    
    def get_for_file(self, file_path: str) -> Optional[FileEmbedder]:
        """Get the appropriate embedder for a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            FileEmbedder: The embedder that can handle this file,
            or None if no suitable embedder is found
        """
        ext = os.path.splitext(file_path)[1].lower()
        if ext in self._extension_map:
            return self._embedders[self._extension_map[ext]]
        
        # Try to find an embedder that can handle this file
        for name, embedder in self._embedders.items():
            if embedder.can_embed(file_path):
                return embedder
        
        return None
    
    def get_all_supported_extensions(self) -> Set[str]:
        """Get all extensions supported by registered embedders."""
        return set(self._extension_map.keys())
    
    def list_embedders(self) -> List[str]:
        """List all registered embedders."""
        return list(self._embedders.keys())
    
    def has_embedder_for(self, file_path: str) -> bool:
        """Check if there's an embedder available for the given file."""
        return self.get_for_file(file_path) is not None

# Global registry instance
registry = EmbedderRegistry()

================
File: src/lfind/embedding/service.py
================
import os
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple

from .registry import registry
from .models import get_model, EmbeddingModel
from ..embed_manager import EmbedManager

class EmbeddingService:
    """Service for embedding files and managing embeddings."""
    
    def __init__(
        self, 
        model_provider: str = "sentence-transformers",
        model_name: str = "all-MiniLM-L6-v2",
        embed_manager: Optional[EmbedManager] = None,
        metric: str = "cosine"
    ):
        """Initialize the embedding service.
        
        Args:
            model_provider: Provider of the embedding model
            model_name: Name of the specific model to use
            embed_manager: Optional EmbedManager instance
            metric: Distance metric for embedding comparisons
        """
        # Initialize the embedding model
        self.model = get_model(model_provider, model_name)
        
        # Create or use the provided EmbedManager
        if embed_manager:
            self.embed_manager = embed_manager
        else:
            dim = self.model.get_embedding_dimension()
            self.embed_manager = EmbedManager(dim=dim, metric=metric)
    
    def embed_file(self, file_path: str, embedding_type: str = 'content', max_length: int = 10000) -> Tuple[Optional[np.ndarray], Dict[str, Any]]:
        """Embed a file with better error handling"""
        try:
            # Get the appropriate embedder
            embedder = registry.get_for_file(file_path)
            if not embedder:
                return None, {"error": f"No suitable embedder found for file: {file_path}"}
            
            # Extract text
            try:
                text = embedder.extract_text(file_path, max_length)
            except Exception as e:
                return None, {"error": f"Failed to extract text from {file_path}: {str(e)}"}
                
            # Get embedding
            try:
                embedding = self.embed_model.get_embedding(text)
                return embedding, {"success": True, "file_path": file_path}
            except Exception as e:
                return None, {"error": f"Failed to generate embedding for {file_path}: {str(e)}"}
                
        except Exception as e:
            return None, {"error": f"Unexpected error embedding {file_path}: {str(e)}"}
    
    def batch_embed_files(
        self, 
        file_paths: List[str]
    ) -> Dict[str, Dict[str, Any]]:
        """Create embeddings for multiple files.
        
        Args:
            file_paths: List of file paths to embed
            
        Returns:
            Dict mapping file paths to dicts with 'embedding' and 'metadata' keys
        """
        results = {}
        for file_path in file_paths:
            embedding, metadata = self.embed_file(file_path)
            if embedding is not None:
                results[file_path] = {
                    'embedding': embedding,
                    'metadata': metadata
                }
        return results
    
    def embed_query(self, query: str) -> np.ndarray:
        """Create an embedding for a search query.
        
        Args:
            query: Search query text
            
        Returns:
            Embedding vector
        """
        return self.model.get_embedding(query)
    
    def search_similar(
        self, 
        query: str, 
        k: int = 10
    ) -> List[Tuple[int, float]]:
        """Search for embeddings similar to the query.
        
        Args:
            query: Text query
            k: Number of results to return
            
        Returns:
            List of (id, similarity_score) tuples
        """
        # Create query embedding
        query_embedding = self.embed_query(query)
        
        # Search in FAISS index
        distances, indices = self.embed_manager.search(query_embedding, k)
        
        # Convert to list of (id, distance) pairs
        results = [
            (int(idx), float(dist)) 
            for idx, dist in zip(indices[0], distances[0])
        ]
        
        return results

================
File: src/lfind/faiss_utils.py
================
import faiss
import numpy as np

def get_faiss_index(dim: int, metric: str = 'cosine', use_gpu: bool = True) -> faiss.Index:
    """
    Create a FAISS index with GPU support if available.
    
    Args:
        dim: Dimension of the embeddings
        metric: Distance metric ('cosine', 'l2', or 'ip')
        use_gpu: Whether to use GPU acceleration if available
        
    Returns:
        FAISS index instance
    """
    # Create the appropriate index based on the metric
    if metric == 'l2':
        index = faiss.IndexFlatL2(dim)
    elif metric == 'ip':
        index = faiss.IndexFlatIP(dim)
    elif metric == 'cosine':
        index = faiss.IndexFlatIP(dim)
    else:
        raise ValueError(f"Unsupported metric: {metric}")
    
    # Try to use GPU if requested
    if use_gpu:
        try:
            # Check if GPU resources are available
            ngpus = faiss.get_num_gpus()
            if ngpus > 0:
                gpu_resources = []
                for i in range(ngpus):
                    res = faiss.StandardGpuResources()
                    gpu_resources.append(res)
                
                # Use the first GPU for simplicity
                # For multiple GPUs, consider faiss.index_cpu_to_all_gpus
                index = faiss.index_cpu_to_gpu(gpu_resources[0], 0, index)
                print(f"FAISS index is using GPU acceleration")
        except Exception as e:
            print(f"GPU acceleration requested but failed: {e}")
            print("Falling back to CPU index")
    
    return index

def normalize_vectors(vectors: np.ndarray) -> np.ndarray:
    """
    Normalize vectors to unit length for cosine similarity search.
    
    Args:
        vectors: Input vectors as numpy array
        
    Returns:
        Normalized vectors
    """
    norm = np.linalg.norm(vectors, axis=1, keepdims=True)
    # Add small epsilon to avoid division by zero
    return vectors / (norm + 1e-10)

================
File: src/lfind/index_manager.py
================
"""
index_manager.py

This module provides functionality to update the file index stored in the SQLite database,
build a hierarchical tree representation from the indexed file metadata, and generate a
compact output for display.
"""

import os
import fnmatch

from .db_manager import DatabaseManager


def should_ignore(name, ignore_patterns):
    """
    Check if a file or directory name should be ignored based on the provided patterns.

    Parameters:
        name (str): The file or directory name.
        ignore_patterns (list of str): Patterns to match against.

    Returns:
        bool: True if the name matches any pattern; otherwise, False.
    """
    return any(fnmatch.fnmatch(name, pattern) for pattern in ignore_patterns)


def update_index(start_path, ignore_patterns, db_path=None):
    """
    Update the file index in the SQLite database by scanning the file system starting at start_path.

    This function performs the following steps:
      1. Resets the 'seen' flags for all records in the database.
      2. Walks the file system (using os.walk) and for each directory and file that is not ignored:
         - Inserts or updates the record (which automatically marks it as seen).
      3. After scanning, removes records that were not touched during the scan.

    Parameters:
        start_path (str): The root directory from which to start indexing.
        ignore_patterns (list of str): Patterns for file/directory names to ignore.
        db_path (str, optional): Path to the database file. If None, use default.
    """
    try:
        from tqdm import tqdm
    except ImportError:
        tqdm = lambda x, **kwargs: x  # Simple passthrough if tqdm is not available
    
    db = DatabaseManager(db_path) if db_path else DatabaseManager()
    try:
        db.reset_seen_flags()
        
        # Count total files first for progress bar
        total_files = 0
        for _, _, files in os.walk(start_path):
            total_files += len(files)
        
        # Process with progress bar
        with tqdm(total=total_files, desc="Indexing files") as pbar:
            for root, dirs, files in os.walk(start_path):
                # Filter out directories to skip based on ignore patterns
                dirs[:] = [d for d in dirs if not should_ignore(d, ignore_patterns)]
                
                # Process each file in the current directory
                for file in files:
                    if not should_ignore(file, ignore_patterns):
                        file_path = os.path.join(root, file)
                        
                        # Get file stats
                        try:
                            stats = os.stat(file_path)
                            
                            # Create file metadata
                            file_data = {
                                "name": file,
                                "absolute_path": file_path,
                                "type": "file",
                                "size": stats.st_size,
                                "modified_at": stats.st_mtime
                            }
                            
                            # Update the database (this also marks the file as "seen")
                            db.touch_file(file_data)
                        except Exception as e:
                            print(f"Error processing file {file_path}: {e}")
                        
                        pbar.update(1)
        
        # Delete missing files
        deleted_count = db.delete_missing_files()
        print(f"Deleted {deleted_count} missing records from the index.")
        
        return True
    finally:
        # Make sure to close the database connection even if an exception occurs
        db.close()


def build_index_tree(directory, extensions=None):
    """
    Build a hierarchical tree representation of the indexed files starting from the given directory.

    This function queries the SQLite database for records whose absolute paths fall under the specified
    directory (optionally filtered by file extensions) and then builds a nested dictionary representing
    the directory structure.

    Parameters:
        directory (str): The root directory to build the tree from.
        extensions (list of str, optional): List of file extensions to filter (e.g., [".pdf", ".docx"]).
        
    Returns:
        dict: A nested dictionary representing the file tree.
    """
    db = DatabaseManager()
    records = db.get_files_by_criteria(directory=directory, extensions=extensions)
    db.close()

    root_abs = os.path.abspath(directory)
    tree = {
        "name": os.path.basename(root_abs) if os.path.basename(root_abs) else root_abs,
        "absolute_path": root_abs,
        "type": "directory",
        "children": []
    }

    for record in records:
        try:
            rel_path = os.path.relpath(record["absolute_path"], root_abs)
        except ValueError:
            # Skip records that are not under the given directory.
            continue
        parts = rel_path.split(os.sep)
        insert_into_tree(tree, parts, record)

    return tree


def insert_into_tree(tree, parts, record):
    """
    Recursively insert a record into the hierarchical tree based on the provided relative path parts.

    Parameters:
        tree (dict): The current tree node.
        parts (list of str): The remaining parts of the file path relative to the tree root.
        record (dict): The file metadata record to insert.
    """
    if not parts:
        return

    if len(parts) == 1:
        # Base case: add the file/directory as a child if not already present.
        for child in tree.get("children", []):
            if child["name"] == record["name"]:
                return
        tree.setdefault("children", []).append(record)
    else:
        # Recursive case: process the subdirectory.
        subdir = parts[0]
        sub_node = None
        for child in tree.get("children", []):
            if child["name"] == subdir and child["type"] == "directory":
                sub_node = child
                break
        if sub_node is None:
            sub_node = {
                "name": subdir,
                "absolute_path": os.path.join(tree["absolute_path"], subdir),
                "type": "directory",
                "children": []
            }
            tree.setdefault("children", []).append(sub_node)
        insert_into_tree(sub_node, parts[1:], record)


def build_index_output(tree, settings):
    """
    Generate a compact, human-friendly output from the hierarchical index tree.

    The settings dictionary may include:
      - max_entries: Maximum number of entries to display per directory (default: 100)
      - include_empty_dirs: Boolean flag indicating whether to display directories even if empty
      - ext_filters: (Optional) List of allowed file extensions (e.g., [".pdf", ".docx"])

    Returns:
        tuple: (output_lines, abs_paths)
            - output_lines (list of str): The compact text representation of the tree.
            - abs_paths (list of str): List of absolute file paths corresponding to the displayed files.
    """
    abs_paths = []

    def traverse(node):
        if not node:
            return []

        if node["type"] == "directory":
            children = node.get("children", [])
            collected = []
            count = 0
            for child in children:
                child_output = traverse(child)
                if child_output:
                    collected.extend(child_output)
                    count += 1
                    if count >= settings.get("max_entries", 100):
                        break
            output = []
            if collected or settings.get("include_empty_dirs", False):
                output.append(f"<Dir: {node['name']}>")
                output.extend(collected)
                output.append("</Dir>")
            return output

        elif node["type"] == "file":
            ext_filters = settings.get("ext_filters")
            if ext_filters:
                _, ext = os.path.splitext(node["name"])
                if ext.lower() not in ext_filters:
                    return []
            abs_paths.append(node["absolute_path"])
            return [node["name"]]

        return []
    
    output_lines = traverse(tree)
    return output_lines, abs_paths

================
File: src/lfind/llm_service.py
================
import os
from typing import List, Optional, Dict

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


class LLMClient:
    """Represents a single LLM client configuration"""
    def __init__(self, provider: str, model: str, api_base: Optional[str]):
        self.provider = provider
        self.model = model
        self.api_base = api_base
        self.client = OpenAI(
            base_url=api_base,
            api_key='ollama' if provider == "ollama" else os.getenv("OPENAI_API_KEY")
        )

    def complete(self, messages: List[Dict[str, str]]) -> str:
        """Send completion request to the LLM."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error during {self.provider} API call: {e}")
            return ""


class LLMService:
    def __init__(self, config: Dict):
        """Initialize LLM service with configuration."""
        self.config = config

        # Initialize default and hard clients
        default_config = config.get("llm_default", {})
        hard_config = config.get("llm_hard", {})

        self.default_client = LLMClient(
            provider=default_config.get("provider", "ollama"),
            model=default_config.get("model", "qwen2.5:14b-instruct-q6_K"),
            api_base=default_config.get("api_base", "http://localhost:11434/v1")
        )

        self.hard_client = LLMClient(
            provider=hard_config.get("provider", "openai"),
            model=hard_config.get("model", "gpt-4o"),
            api_base=hard_config.get("api_base")
        )

    def create_search_prompt(self, query: str, file_list: List[str]) -> str:
        """Create a prompt for file search."""
        file_list_str = '\n'.join(file_list)
        prompt_template = """Given the following list of files:

{}

Find files that best match this search query: "{}"

Instructions:
- Return ONLY the matching filenames, one per line
- Do not include any explanations or additional text
- Do not include directory structures or absolute paths
- If no files match, return an empty response"""
        return prompt_template.format(file_list_str, query)

    def search_files(self, query: str, file_list: List[str], use_hard_model: bool = False) -> List[str]:
        """Search for files using LLM."""
        client = self.hard_client if use_hard_model else self.default_client

        messages = [
            {
                "role": "system",
                "content": "You are a file search assistant that helps find relevant files based on natural language queries."
            },
            {
                "role": "user",
                "content": self.create_search_prompt(query, file_list)
            }
        ]

        print(f"Using {client.provider} model '{client.model}' for search")

        response = client.complete(messages)

        # Split response into lines and filter out empty lines
        results = [line.strip() for line in response.split('\n') if line.strip()]
        return results

    def get_absolute_paths(self, filenames: List[str], abs_paths: List[str]) -> List[str]:
        """Match filenames to their absolute paths."""
        results = []
        for filename in filenames:
            for abs_path in abs_paths:
                if os.path.basename(abs_path) == filename:
                    results.append(abs_path)
                    break
        return results

================
File: src/lfind/main.py
================
import os
import sys
import argparse
from datetime import datetime, timedelta
from typing import Dict, Any

from . import config
from .db_manager import DatabaseManager
from .embedding.service import EmbeddingService
from .llm_service import LLMService
from .search_pipeline import SearchPipeline
from . import index_manager

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# Version information
__version__ = "0.2.0"
__author__ = "Mahrkeenerh"
__description__ = "Natural language file search tool with LLM support"


def print_version():
    """Print version information and exit."""
    print(f"{__description__} v{__version__}")
    print(f"Author: {__author__}")

    config_path = config.get_config_location()
    print(f"Config file location: {config_path}")


def parse_date(date_str: str) -> datetime:
    """Parse a date string in YYYY-MM-DD format."""
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid date format: {date_str}. Use YYYY-MM-DD.")


def parse_size(size_str: str) -> int:
    """Parse a size string like '5MB' into bytes."""
    if not size_str:
        return 0
        
    size_str = size_str.upper()
    units = {
        'B': 1,
        'KB': 1024,
        'MB': 1024 * 1024,
        'GB': 1024 * 1024 * 1024,
        'TB': 1024 * 1024 * 1024 * 1024
    }
    
    # Handle different formats: 5MB, 5 MB
    size_str = size_str.replace(" ", "")
    
    # Find the unit
    unit = 'B'
    for u in units:
        if size_str.endswith(u):
            unit = u
            size_str = size_str[:-len(u)]
            break
    
    try:
        size_value = float(size_str)
        return int(size_value * units[unit])
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid size format: {size_str}{unit}")


def setup_common_arguments(parser):
    """Add common arguments to a parser or subparser."""
    parser.add_argument("-d", "--directory", default=".", help="Search directory path")
    parser.add_argument("-i", "--ignore", nargs="+", help="Additional ignore patterns")
    parser.add_argument("--ignore-defaults", action="store_true", help="Ignore default patterns")
    parser.add_argument("--config", type=str, help="Path to configuration file")


def setup_search_parser(subparsers):
    """Set up the search subparser with all relevant arguments."""
    search_parser = subparsers.add_parser("search", help="Search for files")
    search_parser.add_argument("query", help="Natural language query to search for files")
    setup_common_arguments(search_parser)
    
    # File filtering options
    search_parser.add_argument("-e", "--extensions", nargs="+", help="Only include files with specified extensions")
    
    # Date filtering options
    search_parser.add_argument("--created-after", type=parse_date, help="Filter files created after date (YYYY-MM-DD)")
    search_parser.add_argument("--created-before", type=parse_date, help="Filter files created before date (YYYY-MM-DD)")
    search_parser.add_argument("--modified-after", type=parse_date, help="Filter files modified after date (YYYY-MM-DD)")
    search_parser.add_argument("--modified-before", type=parse_date, help="Filter files modified before date (YYYY-MM-DD)")
    
    # Size filtering options
    search_parser.add_argument("--min-size", type=parse_size, help="Minimum file size (e.g., '5MB', '1KB')")
    search_parser.add_argument("--max-size", type=parse_size, help="Maximum file size (e.g., '100MB', '2GB')")
    
    # Search options
    search_parser.add_argument("-r", "--refresh-cache", action="store_true", help="Force rebuild the cache")
    search_parser.add_argument("-H", "--hard", action="store_true", help="Use the more powerful LLM model")
    search_parser.add_argument("--semantic", action="store_true", help="Use semantic search")
    search_parser.add_argument("--no-llm", action="store_true", help="Disable LLM search")
    search_parser.add_argument("--top", type=int, default=10, help="Number of top results to show")
    
    return search_parser


def setup_embed_parser(subparsers):
    """Set up the embed subparser for embedding management."""
    embed_parser = subparsers.add_parser("embed", help="Manage file embeddings")
    setup_common_arguments(embed_parser)
    embed_parser.add_argument("-r", "--rebuild", action="store_true", help="Rebuild all embeddings")
    embed_parser.add_argument("-u", "--update", action="store_true", help="Update only changed files")
    embed_parser.add_argument("-t", "--types", nargs="+", default=["text", "pdf"], 
                            help="File types to embed (e.g., text, pdf, docx)")
    embed_parser.add_argument("--batch-size", type=int, default=100, 
                            help="Number of files to process in one batch")
    return embed_parser


def setup_index_parser(subparsers):
    """Set up the index subparser for database management."""
    index_parser = subparsers.add_parser("index", help="Manage the file index")
    setup_common_arguments(index_parser)
    index_parser.add_argument("--rebuild", action="store_true", 
                             help="Force rebuild the entire index")
    index_parser.add_argument("--status", action="store_true",
                             help="Show index statistics and status")
    index_parser.add_argument("--cleanup", action="store_true",
                             help="Clean up missing files from the index")
    return index_parser


def setup_setup_parser(subparsers):
    """Set up the setup subparser for initial configuration."""
    setup_parser = subparsers.add_parser("setup", help="Initial setup and configuration")
    setup_common_arguments(setup_parser)
    setup_parser.add_argument("--model", choices=["openai", "sentence-transformers"], 
                             default="sentence-transformers", help="Embedding model to use")
    setup_parser.add_argument("--llm-provider", choices=["openai", "ollama"], 
                             default="ollama", help="LLM provider for search")
    return setup_parser


def handle_search_command(args, cfg):
    """Handle the search command."""
    # Set up paths
    cache_dir = cfg["cache_dir"]
    os.makedirs(cache_dir, exist_ok=True)
    db_path = os.path.join(cache_dir, "metadata.db")

    # Process arguments and config
    ignore_patterns = list(cfg["ignore_patterns"])
    if args.ignore_defaults:
        ignore_patterns = []
    if args.ignore:
        ignore_patterns.extend(args.ignore)

    ext_filters = None
    if args.extensions:
        ext_filters = [
            f".{ext.lower()}" if not ext.startswith('.') else ext.lower() 
            for ext in args.extensions
        ]

    # Process directory
    start_path = os.path.abspath(args.directory)
    if not os.path.exists(start_path):
        print(f"Error: Path '{start_path}' does not exist.", file=sys.stderr)
        sys.exit(1)

    # Initialize database manager
    db = DatabaseManager(db_path)
    
    # Update the index if needed or requested
    if args.refresh_cache:
        print("Rebuilding file index...")
        index_manager.update_index(start_path, ignore_patterns)
    
    # Initialize the embedding service
    try:
        embedding_service = EmbeddingService()
        print("Initialized embedding service")
    except Exception as e:
        if args.semantic:
            print(f"Error initializing embedding service: {e}", file=sys.stderr)
            print("Semantic search will be disabled.", file=sys.stderr)
            args.semantic = False
        embedding_service = None

    # Initialize LLM service
    llm = LLMService(cfg)

    if args.hard and not cfg.get("llm_hard"):
        print("Warning: Hard model not specified in config, using default model", file=sys.stderr)

    # Create search pipeline
    search_pipeline = SearchPipeline(db, embedding_service, llm, cfg)
    
    # Build filter criteria
    filter_criteria = {}
    
    # Add date filters
    if args.created_after:
        filter_criteria["created_after"] = args.created_after
    if args.created_before:
        filter_criteria["created_before"] = args.created_before
    if args.modified_after:
        filter_criteria["modified_after"] = args.modified_after
    if args.modified_before:
        filter_criteria["modified_before"] = args.modified_before
        
    # Add size filters
    if args.min_size is not None:
        filter_criteria["min_size"] = args.min_size
    if args.max_size is not None:
        filter_criteria["max_size"] = args.max_size
    
    # Perform search
    results = search_pipeline.multi_search(
        query=args.query,
        directory=start_path,
        extensions=ext_filters,
        use_semantic=args.semantic,
        use_llm=not args.no_llm,
        use_hard_llm=args.hard,
        top_k=args.top,
        filter_criteria=filter_criteria
    )
    
    # Display results
    if results:
        print(f"\nFound {len(results)} matching files:")
        for result in results:
            path = result.get('absolute_path', '')
            if path:
                print(path)
    else:
        print("\nNo matching files found.")

    # Close database connection
    db.close()


def handle_embed_command(args, cfg):
    """Handle the embed command."""
    print("Managing embeddings...")
    
    # Set up paths
    cache_dir = cfg["cache_dir"]
    os.makedirs(cache_dir, exist_ok=True)
    db_path = os.path.join(cache_dir, "metadata.db")
    
    # Process directory
    start_path = os.path.abspath(args.directory)
    if not os.path.exists(start_path):
        print(f"Error: Path '{start_path}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    # Initialize database and embedding service
    db = DatabaseManager(db_path)
    
    try:
        embedding_service = EmbeddingService()
        print("Initialized embedding service")
    except Exception as e:
        print(f"Error initializing embedding service: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Get files to process
    if args.rebuild:
        print("Rebuilding all embeddings...")
        files_to_process = db.get_files_by_criteria(start_path, type="file")
    else:
        # Get only files that have changed since last embedding
        print("Updating embeddings for changed files...")
        files_to_process = db.get_files_by_criteria(
            start_path, 
            type="file",
            needs_embedding_update=True
        )
    
    if not files_to_process:
        print("No files need embedding updates.")
        db.close()
        return
    
    print(f"Found {len(files_to_process)} files to process")
    
    # Process files in batches with progress display
    batch_size = args.batch_size
    total_batches = (len(files_to_process) + batch_size - 1) // batch_size
    
    successful = 0
    failed = 0
    
    for i in range(0, len(files_to_process), batch_size):
        batch = files_to_process[i:i+batch_size]
        print(f"Processing batch {(i // batch_size) + 1}/{total_batches} ({len(batch)} files)")
        
        for file in tqdm(batch) if HAS_TQDM else batch:
            file_path = file.get("absolute_path")
            if not file_path or not os.path.exists(file_path):
                failed += 1
                continue
                
            try:
                embedding, metadata = embedding_service.embed_file(file_path)
                if embedding is not None:
                    # Save embedding to database
                    db.update_embedding(file["id"], embedding)
                    successful += 1
                else:
                    print(f"Failed to generate embedding for {file_path}: {metadata.get('error', 'Unknown error')}")
                    failed += 1
            except Exception as e:
                print(f"Error processing {file_path}: {str(e)}")
                failed += 1
    
    print(f"Embedding processing complete: {successful} successful, {failed} failed")
    db.close()


def handle_index_command(args, cfg):
    """Handle the index command."""
    # Set up paths
    cache_dir = cfg["cache_dir"]
    os.makedirs(cache_dir, exist_ok=True)
    db_path = os.path.join(cache_dir, "metadata.db")
    
    # Process arguments and config
    ignore_patterns = list(cfg["ignore_patterns"])
    if args.ignore_defaults:
        ignore_patterns = []
    if args.ignore:
        ignore_patterns.extend(args.ignore)
    
    # Process directory
    start_path = os.path.abspath(args.directory)
    if not os.path.exists(start_path):
        print(f"Error: Path '{start_path}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    # Initialize database manager
    db = DatabaseManager(db_path)
    
    if args.rebuild:
        print("Rebuilding file index...")
        index_manager.update_index(start_path, ignore_patterns)
    elif args.cleanup:
        print("Cleaning up missing files from index...")
        db.reset_seen_flags()
        deleted_count = db.delete_missing_files()
        print(f"Deleted {deleted_count} missing records from the index.")
    elif args.status:
        # Show index statistics
        file_count = db.get_file_count()
        dir_count = db.get_directory_count()
        last_update = db.get_last_update_time()
        
        print("Index status:")
        print(f"  Total files indexed: {file_count}")
        print(f"  Total directories indexed: {dir_count}")
        if last_update:
            print(f"  Last index update: {last_update}")
        else:
            print("  Last index update: Never")
    else:
        print("No index operation specified. Use --rebuild, --cleanup, or --status.")
    
    db.close()


def handle_setup_command(args, cfg):
    """Handle the setup command."""
    print("Running initial setup...")
    
    # Update configuration
    updated_cfg = dict(cfg)
    if args.model:
        updated_cfg["embedding_model"] = args.model
    if args.llm_provider:
        updated_cfg["llm_provider"] = args.llm_provider
    
    # Save updated config
    config_path = config.get_user_config_path()
    with open(config_path, 'w') as f:
        import json
        json.dump(updated_cfg, f, indent=2)
    
    print(f"Configuration saved to {config_path}")
    
    # Set up paths
    cache_dir = updated_cfg["cache_dir"]
    os.makedirs(cache_dir, exist_ok=True)
    db_path = os.path.join(cache_dir, "metadata.db")
    
    # Process directory
    start_path = os.path.abspath(args.directory)
    if not os.path.exists(start_path):
        print(f"Error: Path '{start_path}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    # Initialize database
    print("Initializing database...")
    db = DatabaseManager(db_path)
    
    # Build initial index
    print("Building initial file index...")
    ignore_patterns = updated_cfg["ignore_patterns"]
    if args.ignore:
        ignore_patterns.extend(args.ignore)
    if args.ignore_defaults:
        ignore_patterns = []
        
    index_manager.update_index(start_path, ignore_patterns)
    
    print("Setup complete! You can now use lfind to search for files.")
    db.close()


def main():
    """Main entry point for the lfind command."""
    config.ensure_user_config()

    # Create main parser
    parser = argparse.ArgumentParser(
        description="Search for files using natural language queries with LLM support."
    )
    parser.add_argument("-v", "--version", action="store_true", help="Show version information and exit")
    
    # Create subparsers
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    
    # Set up command parsers
    search_parser = setup_search_parser(subparsers)
    embed_parser = setup_embed_parser(subparsers)
    index_parser = setup_index_parser(subparsers)
    setup_parser = setup_setup_parser(subparsers)
    
    # Backwards compatibility: Allow search as default command
    parser.add_argument("query", nargs="?", help="Natural language query to search for files (shorthand for search command)")
    parser.add_argument("-d", "--directory", default=".", help="Search directory path")
    parser.add_argument("-e", "--extensions", nargs="+", help="Only include files with specified extensions")
    parser.add_argument("-i", "--ignore", nargs="+", help="Additional ignore patterns")
    parser.add_argument("--ignore-defaults", action="store_true", help="Ignore default patterns")
    parser.add_argument("-r", "--refresh-cache", action="store_true", help="Force rebuild the cache")
    parser.add_argument("--config", type=str, help="Path to configuration file")
    parser.add_argument("-H", "--hard", action="store_true", help="Use the more powerful LLM model")
    parser.add_argument("--semantic", action="store_true", help="Use semantic search")
    parser.add_argument("--no-llm", action="store_true", help="Disable LLM search")
    parser.add_argument("--top", type=int, default=10, help="Number of top results to show")
    
    # Parse arguments
    args = parser.parse_args()

    # Handle version flag
    if args.version:
        print_version()
        sys.exit(0)
    
    # Load configuration
    cfg = config.load_config(args.config)
    
    # Handle commands
    if args.command == "search":
        handle_search_command(args, cfg)
    elif args.command == "embed":
        handle_embed_command(args, cfg)
    elif args.command == "index":
        handle_index_command(args, cfg)
    elif args.command == "setup":
        handle_setup_command(args, cfg)
    elif args.query:  # Default to search if a query is provided
        # For compatibility with old CLI, execute search
        handle_search_command(args, cfg)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: src/lfind/requirements.txt
================
openai>=1.61.1
python-dotenv>=1.0.1

================
File: src/lfind/search_pipeline.py
================
from typing import List, Dict, Any, Optional, Set, Tuple
import os

from .db_manager import DatabaseManager
from .embed_manager import EmbedManager
from .embedding.service import EmbeddingService
from .llm_service import LLMService

class SearchPipeline:
    """Multi-stage search pipeline with history support."""
    
    def __init__(
        self,
        db_manager: DatabaseManager,
        embedding_service: EmbeddingService,
        llm_service: LLMService,
        config: Dict[str, Any]
    ):
        """Initialize the search pipeline.
        
        Args:
            db_manager: Database manager for file metadata
            embedding_service: Service for creating and comparing embeddings
            llm_service: Service for LLM-based search
            config: Application configuration
        """
        self.db = db_manager
        self.embedding = embedding_service
        self.llm = llm_service
        self.config = config
        self.search_history = []
    
    def add_to_history(
        self, 
        query: str, 
        results: List[Dict[str, Any]], 
        search_type: str,
        params: Dict[str, Any]
    ) -> None:
        """Add search results to history.
        
        Args:
            query: The search query
            results: The search results
            search_type: Type of search (extension, semantic, llm)
            params: Additional parameters for the search
        """
        self.search_history.append({
            'query': query,
            'results': results,
            'search_type': search_type,
            'params': params,
            'result_count': len(results)
        })
    
    def filter_by_extension(
        self, 
        extensions: List[str], 
        directory: Optional[str] = None,
        previous_results: Optional[List[Dict[str, Any]]] = None
    ) -> List[Dict[str, Any]]:
        """Filter files by extension.
        
        Args:
            extensions: List of file extensions to include (e.g. [".py", ".txt"])
            directory: Optional directory to search in
            previous_results: Optional previous search results to filter
            
        Returns:
            List of matching file records
        """
        # Ensure extensions have a leading dot and are lowercase
        normalized_extensions = [
            ext.lower() if ext.startswith(".") else f".{ext.lower()}" 
            for ext in extensions
        ]
        
        if previous_results:
            # Filter the previous results
            filtered_results = []
            for record in previous_results:
                file_name = record.get('name', '')
                _, ext = os.path.splitext(file_name)
                if ext.lower() in normalized_extensions:
                    filtered_results.append(record)
            results = filtered_results
        else:
            # Get fresh results from the database
            results = self.db.get_files_by_criteria(
                extensions=normalized_extensions,
                directory=directory,
                file_type="file"
            )
        
        self.add_to_history(
            query=f"extension:{','.join(extensions)}",
            results=results,
            search_type="extension",
            params={
                'extensions': extensions,
                'directory': directory
            }
        )
        
        return results
    
    def search_semantic(
        self,
        query: str,
        top_k: int = 10,
        directory: Optional[str] = None,
        extensions: Optional[List[str]] = None,
        previous_results: Optional[List[Dict[str, Any]]] = None
    ) -> List[Dict[str, Any]]:
        """Search files by semantic similarity.
        
        Args:
            query: The search query
            top_k: Number of results to return
            directory: Optional directory to search in
            extensions: Optional list of file extensions to filter
            previous_results: Optional previous search results to filter
            
        Returns:
            List of matching file records
        """
        # Create query embedding
        query_embedding = self.embedding.embed_query(query)
        
        # Choose approach based on the size of the candidate set
        if previous_results and len(previous_results) < 1000:
            # Limited Index Approach: Create a temporary index for the candidate set
            
            # Get file IDs from previous results
            file_ids = [r['id'] for r in previous_results if 'id' in r]
            
            if len(file_ids) == 0:
                return []
            
            # Create temporary index for these files
            temp_manager = EmbedManager(
                dim=self.embedding.embed_manager.dim,
                metric=self.embedding.embed_manager.metric
            )
            
            # Get embeddings for candidate files
            embeddings = []
            matched_ids = []
            
            for file_record in previous_results:
                embedding_id = file_record.get('embedding_id')
                file_id = file_record.get('id')
                
                if embedding_id is not None and file_id is not None:
                    # Get the embedding from the database or embedding service
                    file_path = file_record.get('absolute_path')
                    emb, _ = self.embedding.embed_file(file_path)
                    if emb is not None:
                        embeddings.append(emb)
                        matched_ids.append(file_id)
            
            if not embeddings:
                return []
                
            # Add embeddings to temporary index
            temp_manager.add_embeddings(embeddings)
            
            # Search in temporary index
            distances, indices = temp_manager.search(query_embedding, k=min(top_k, len(embeddings)))
            
            # Map results back to file records
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(matched_ids):
                    file_id = matched_ids[idx]
                    for record in previous_results:
                        if record.get('id') == file_id:
                            results.append(record)
                            break
            
        else:
            # Post-Filtering Approach: Search full index and filter
            # This approach is used when there are no previous results or too many
            
            # Get candidate set through basic filtering if needed
            candidate_results = previous_results
            if not candidate_results:
                candidate_results = self.db.get_files_by_criteria(
                    directory=directory,
                    extensions=extensions,
                    file_type="file"
                )
            
            # Search the full index
            similarity_results = self.embedding.search_similar(query, k=top_k * 10)  # Get more results to filter
            
            # Filter to intersection with candidate set
            candidate_ids = {r['id'] for r in candidate_results if 'id' in r}
            results = []
            
            for idx, score in similarity_results:
                file_record = self.db.get_file_by_id(idx)
                if file_record and file_record.get('id') in candidate_ids:
                    results.append(file_record)
                    
                if len(results) >= top_k:
                    break
        
        # Add to history
        self.add_to_history(
            query=query,
            results=results,
            search_type="semantic",
            params={
                'top_k': top_k,
                'directory': directory,
                'extensions': extensions
            }
        )
        
        return results
    
    def search_llm(
        self,
        query: str,
        directory: Optional[str] = None,
        extensions: Optional[List[str]] = None,
        use_hard_model: bool = False,
        previous_results: Optional[List[Dict[str, Any]]] = None
    ) -> List[Dict[str, Any]]:
        """Search files using LLM-based natural language matching.
        
        Args:
            query: The natural language search query
            directory: Optional directory to search in
            extensions: Optional list of file extensions to filter
            use_hard_model: Whether to use the more powerful LLM model
            previous_results: Optional previous search results to filter
            
        Returns:
            List of matching file records
        """
        # Get candidate files either from previous results or a new database query
        candidate_records = previous_results
        if not candidate_records:
            candidate_records = self.db.get_files_by_criteria(
                directory=directory,
                extensions=extensions,
                file_type="file"
            )
        
        if not candidate_records:
            return []
        
        # Extract filenames from records for LLM search
        file_names = [record.get('name', '') for record in candidate_records]
        
        # Perform LLM search
        matching_names = self.llm.search_files(query, file_names, use_hard_model)
        
        # Match results to file records
        results = []
        for name in matching_names:
            for record in candidate_records:
                if record.get('name') == name:
                    results.append(record)
                    break
        
        # Add to history
        self.add_to_history(
            query=query,
            results=results,
            search_type="llm",
            params={
                'directory': directory,
                'extensions': extensions,
                'use_hard_model': use_hard_model
            }
        )
        
        return results
    
    def multi_search(
        self,
        query: str,
        directory: Optional[str] = None,
        extensions: Optional[List[str]] = None,
        use_semantic: bool = True,
        use_llm: bool = True,
        use_hard_llm: bool = False,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """Perform multi-stage search using all available methods.
        
        Args:
            query: The search query
            directory: Optional directory to search in
            extensions: Optional list of file extensions to filter
            use_semantic: Whether to use semantic search
            use_llm: Whether to use LLM search
            use_hard_llm: Whether to use the more powerful LLM model
            top_k: Number of results to return
            
        Returns:
            List of matching file records
        """
        # Stage 1: Basic metadata filtering
        base_results = self.db.get_files_by_criteria(
            directory=directory,
            extensions=extensions,
            file_type="file"
        )
        
        if not base_results:
            return []
        
        results = []
        
        # Stage 2: Semantic search if requested
        if use_semantic:
            semantic_results = self.search_semantic(
                query=query,
                top_k=top_k,
                previous_results=base_results
            )
            results.extend(semantic_results)
        
        # Stage 3: LLM search if requested
        if use_llm:
            llm_results = self.search_llm(
                query=query,
                use_hard_model=use_hard_llm,
                previous_results=base_results
            )
            
            # Add LLM results that weren't already found in semantic search
            result_ids = {r['id'] for r in results if 'id' in r}
            for record in llm_results:
                if record.get('id') not in result_ids:
                    results.append(record)
                    result_ids.add(record.get('id'))
                    
                    # Limit to top_k results
                    if len(results) >= top_k:
                        break
        
        # If neither semantic nor LLM search was used, return metadata results directly
        if not use_semantic and not use_llm:
            results = base_results[:top_k]
        
        # Add combined search to history
        self.add_to_history(
            query=query,
            results=results,
            search_type="multi",
            params={
                'directory': directory,
                'extensions': extensions,
                'use_semantic': use_semantic,
                'use_llm': use_llm,
                'use_hard_llm': use_hard_llm,
                'top_k': top_k
            }
        )
        
        return results[:top_k]

================
File: summary.md
================
## **Project Summary: LFind – A Natural Language File Finder**

### **Overview and Goals**

*LFind* is a file search tool designed to help users locate files and directories across very large repositories (potentially millions of entries) using a combination of classical metadata filtering and advanced semantic search methods. The primary goals are:

1. **Traditional Filtering:**  
   Allow users to quickly narrow down results using standard file attributes such as:
   - Directory (or absolute path) filtering (e.g., search within a given directory recursively)
   - File extensions (e.g., PDF, DOCX)
   - Date ranges (e.g., modified after a specific date)
   - File sizes

2. **Semantic Search:**  
   Enable further refinement of the filtered results using:
   - **Vector-based similarity search (via FAISS):** Compute embeddings from file content, titles, or summaries to capture semantic meaning.
   - **LLM-assisted search:** Use large language models (LLMs) to interpret natural language queries and match them against the candidate set, optionally integrating with semantic vector search.

3. **Ease of Installation & Efficiency:**  
   - Use SQLite as a serverless, lightweight metadata store.
   - Use FAISS for fast, local vector similarity searches.
   - Ensure the overall system is efficient even with large databases and requires minimal extra infrastructure.

---

## **Architecture & Design**

### **1. Metadata Management**

- **Database:**  
  We use SQLite (via Python’s built-in `sqlite3` module) to store file metadata. This database holds all the necessary details for each file or directory:
  - **Fields include:**  
    - `id`: Unique identifier (INTEGER PRIMARY KEY AUTOINCREMENT)
    - `name`: File name  
    - `absolute_path`: Full file path (used for recursive filtering with a `LIKE` query)  
    - `type`: Indicates if the entry is a file or a directory  
    - `extension`: File extension  
    - `size`: File size  
    - `created_at` and `modified_at`: Timestamps  
    - `last_indexed_at`: When the file was last updated in the database  
    - `embedding_id`: An integer linking to the corresponding vector embedding in FAISS (if applicable)  
    - `seen`: A flag used during scheduled updates to track whether the file was encountered in the current scan

- **Single-Pass Update Mechanism:**  
  Instead of maintaining a separate cached file tree and then doing two passes (one for updating and one for cleaning), our design uses a scheduled function that:
  - **Resets all `seen` flags** to 0.
  - **Walks through the file system** (using something like `os.walk`), and for each file/directory:
    - Calls a function (`touch_file`) that compares the current file’s stats (size, modification time) with what's in the database.
    - If the file is new or updated, the record is inserted or updated and the `seen` flag is set to 1.
    - If the record is already up-to-date, only the `seen` flag is set.
  - **After the walk,** a function (`delete_missing_files`) removes all records with `seen = 0` (files that no longer exist).

### **2. Semantic Search with FAISS**

- **Vector Embeddings:**  
  For files that are candidates for semantic search, we compute vector embeddings (e.g., from file content summaries) using a chosen model. These embeddings capture the semantic meaning of the file.

- **FAISS Index:**  
  The embeddings are stored in a FAISS index using an `IndexIDMap`, which maps each embedding to the unique ID from the SQLite metadata store. This allows us to perform fast similarity searches. When a query is made:
  - **Two approaches are considered:**
    - **Limited Index Approach:** Retrieve embeddings for the candidate files (filtered by metadata) and build a temporary FAISS index for a very selective search.
    - **Post-Filtering Approach:** Run a similarity search on the full FAISS index and then filter the returned results based on the metadata candidate set.
  - The choice between these strategies can be tuned based on the selectivity of the metadata filters.

### **3. LLM Integration**

- **Natural Language Query Processing:**  
  In addition to vector similarity search, the system supports searches using large language models. For example, an LLM may be prompted with candidate file summaries (obtained via metadata filtering) to determine which files best match a natural language query.

- **Filtered LLM Search:**  
  This LLM search is applied on the candidate set (as determined by metadata filtering), ensuring that even semantic search via LLMs respects the user-specified criteria (directory, date, size, etc.).

### **4. User Interface**

- **Console/CLI:**  
  A simple console UI allows users to:
  - Enter search criteria (directory, extension, date, size)
  - View initial results from metadata filtering
  - Optionally refine the results with a semantic similarity query (using FAISS and/or LLMs)
  - Display final matching file paths along with relevant metadata

---

## **Implementation Roadmap**

1. **Data Acquisition & Metadata Extraction:**
   - **File System Walk:**  
     Implement a scheduled process that uses `os.walk` (or a similar mechanism) to iterate through the file system.
   - **Database Population:**  
     For each file/directory, build a metadata dictionary and call the `touch_file` method to update or insert the record.  
     Use a `seen` flag in the database to track which records are up-to-date.
   - **Cleanup:**  
     After scanning, call a function to delete all records where `seen` remains 0, indicating that these files were not found.

2. **Metadata Storage Implementation:**
   - Develop the `DatabaseManager` class (as detailed in the provided code), including methods for:
     - Initialization (`_init_database`)
     - Inserting/updating records (`upsert_file` and `touch_file`)
     - Resetting the `seen` flag (`reset_seen_flags`)
     - Deleting missing files (`delete_missing_files`)
     - Querying files by criteria (`get_files_by_criteria`)
     - Retrieving mappings and individual records

3. **FAISS Vector Index Integration:**
   - Compute embeddings for relevant file content.
   - Create and maintain a FAISS index that maps embeddings to file IDs.
   - Develop functions to perform vector similarity searches, either by building a temporary index for a filtered candidate set or by post-filtering results from the full index.

4. **LLM-Based Search Integration:**
   - Extend the existing LLM search functionality to allow natural language queries on top of the metadata-filtered candidate set.
   - Ensure that the LLM prompts include only the relevant context (e.g., file names or summaries) from the filtered set.

5. **User Interface (CLI):**
   - Develop a console interface that guides the user through:
     - Inputting filter criteria.
     - Displaying metadata-filtered results.
     - Allowing additional semantic query input.
     - Displaying final file paths and details.

6. **Testing, Benchmarking, and Documentation:**
   - Write unit and integration tests for each module (metadata handling, FAISS operations, LLM integration).
   - Benchmark the performance of both similarity search approaches (re-indexing versus post-filtering) and optimize accordingly.
   - Prepare documentation that explains installation (using SQLite and FAISS, no external servers needed), configuration, and usage instructions.

7. **Final Integration & Deployment:**
   - Integrate all components into a single cohesive application.
   - Package the project as a Python package or executable for easy distribution and installation.
   - Ensure clear configuration options for scheduled updates, filtering criteria, and search modes.

---

## **Conclusion**

*LFind* aims to provide a fast, flexible, and easy-to-install file search tool that combines the power of metadata filtering (via SQLite) with semantic search techniques (via FAISS and LLMs). The current design leverages a single-pass scheduled update that walks through the file system, updates the database records with a "seen" flag, and then cleans up records for deleted files—all while keeping the installation simple and the system efficient for large-scale file repositories.

================
File: tests/run_all_tests.py
================
# tests/run_all_tests.py
import unittest
import os
import sys

# Add the parent directory (project root) to the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Find all test modules
loader = unittest.TestLoader()
test_suite = loader.discover(os.path.dirname(__file__), pattern="test_*.py")

# Run tests
runner = unittest.TextTestRunner(verbosity=2)
result = runner.run(test_suite)

# Exit with non-zero code if tests failed
sys.exit(not result.wasSuccessful())

================
File: tests/test_basic.py
================
# tests/test_basic.py
import os
import sys
import unittest
import tempfile
import shutil
from pathlib import Path

# Add src directory to path to import lfind
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.lfind.db_manager import DatabaseManager
from src.lfind.config import load_config

class TestBasicFunctionality(unittest.TestCase):
    def setUp(self):
        """Set up test environment"""
        # Create a temporary directory for test files
        self.test_dir = tempfile.mkdtemp()
        
        # Create a temporary database file
        self.db_path = os.path.join(self.test_dir, "test_metadata.db")
        self.db = DatabaseManager(self.db_path)
        
        # Create some test files
        self.create_test_files()
        
    def tearDown(self):
        """Clean up test environment"""
        # Close database connection
        self.db.close()
        
        # Remove temporary directory and all its contents
        shutil.rmtree(self.test_dir)
    
    def create_test_files(self):
        """Create test files in the temporary directory"""
        # Create directory structure
        os.makedirs(os.path.join(self.test_dir, "docs"), exist_ok=True)
        os.makedirs(os.path.join(self.test_dir, "src"), exist_ok=True)
        
        # Create test files with content
        files = [
            ("test.txt", "This is a test file for searching."),
            ("docs/document.md", "# Documentation\nThis is a documentation file."),
            ("src/code.py", "def test():\n    print('Hello, world!')")
        ]
        
        for rel_path, content in files:
            file_path = os.path.join(self.test_dir, rel_path)
            with open(file_path, "w") as f:
                f.write(content)
    
    def test_database_initialization(self):
        """Test that database is properly initialized"""
        # This just checks if the database was created
        self.assertTrue(os.path.exists(self.db_path))
    
    def test_touch_file(self):
        """Test touch_file functionality"""
        # Create file data for a test file
        file_path = os.path.join(self.test_dir, "test.txt")
        file_data = {
            "name": "test.txt",
            "absolute_path": file_path,
            "type": "file"
        }
        
        # Touch file should return True for a new file
        result = self.db.touch_file(file_data)
        self.assertTrue(result)
        
        # Touch file should return False for an unchanged file
        result = self.db.touch_file(file_data)
        self.assertFalse(result)

if __name__ == "__main__":
    unittest.main()

================
File: tests/test_embeddings.py
================
# tests/test_embeddings.py
import os
import sys
import unittest
import tempfile
import numpy as np

# Add src directory to path to import lfind
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.lfind.embed_manager import EmbedManager

class TestEmbeddings(unittest.TestCase):
    def test_embed_manager_basic(self):
        """Test basic functionality of EmbedManager"""
        # Create an EmbedManager with a small dimension for testing
        embed_mgr = EmbedManager(dim=4, metric='ip')
        
        # Create some test embeddings
        embeddings = np.array([
            [1.0, 0.0, 0.0, 0.0],
            [0.0, 1.0, 0.0, 0.0],
            [0.0, 0.0, 1.0, 0.0],
            [0.0, 0.0, 0.0, 1.0]
        ], dtype=np.float32)
        
        # Add embeddings to the index
        embed_mgr.add_embeddings(embeddings)
        
        # Verify count
        self.assertEqual(embed_mgr.total_embeddings(), 4)
        
        # Test search
        query = np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32)
        distances, indices = embed_mgr.search(query, k=2)
        
        # First result should be the first embedding (identical to query)
        self.assertEqual(indices[0][0], 0)
    
    def test_save_load_index(self):
        """Test saving and loading the index"""
        # Create a temporary file for the index
        with tempfile.NamedTemporaryFile(suffix='.index', delete=False) as tmp:
            index_path = tmp.name
        
        try:
            # Create an EmbedManager and add embeddings
            embed_mgr = EmbedManager(dim=4, metric='cosine')
            
            embeddings = np.array([
                [1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0]
            ], dtype=np.float32)
            
            embed_mgr.add_embeddings(embeddings)
            
            # Save the index
            embed_mgr.save_index(index_path)
            
            # Create a new EmbedManager and load the index
            new_embed_mgr = EmbedManager(dim=4, metric='cosine')
            new_embed_mgr.load_index(index_path)
            
            # Verify the loaded index has the same number of embeddings
            self.assertEqual(new_embed_mgr.total_embeddings(), 2)
        finally:
            # Clean up the temporary file
            if os.path.exists(index_path):
                os.unlink(index_path)
    
    def test_cosine_similarity(self):
        """Test cosine similarity in EmbedManager"""
        # Create an EmbedManager with cosine similarity
        embed_mgr = EmbedManager(dim=3, metric='cosine')
        
        # Create some test embeddings with different magnitudes
        embeddings = np.array([
            [3.0, 0.0, 0.0],  # Unit vector in x direction, but magnitude 3
            [0.0, 2.0, 0.0],  # Unit vector in y direction, but magnitude 2
            [0.0, 0.0, 1.0]   # Unit vector in z direction
        ], dtype=np.float32)
        
        # Add embeddings to the index
        embed_mgr.add_embeddings(embeddings)
        
        # Query with a vector in the x direction but with different magnitude
        query = np.array([10.0, 0.0, 0.0], dtype=np.float32)
        distances, indices = embed_mgr.search(query, k=3)
        
        # First result should be the first embedding despite different magnitude
        self.assertEqual(indices[0][0], 0)

if __name__ == "__main__":
    unittest.main()

================
File: tests/test_index_manager.py
================
# tests/test_index_manager.py
import os
import sys
import unittest
import tempfile
import shutil

# Add src directory to path to import lfind
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.lfind.db_manager import DatabaseManager
from src.lfind.index_manager import update_index, should_ignore

class TestIndexManager(unittest.TestCase):
    def setUp(self):
        """Set up test environment"""
        # Create a temporary directory for test files
        self.test_dir = tempfile.mkdtemp()
        
        # Create a temporary database file
        self.db_path = os.path.join(self.test_dir, "test_metadata.db")
        
        # Create some test files and directories
        self.create_test_files()
        
    def tearDown(self):
        """Clean up test environment"""
        # Remove temporary directory and all its contents
        shutil.rmtree(self.test_dir)
    
    def create_test_files(self):
        """Create test files in the temporary directory"""
        # Create directory structure with some files to test ignore patterns
        os.makedirs(os.path.join(self.test_dir, "docs"), exist_ok=True)
        os.makedirs(os.path.join(self.test_dir, ".git"), exist_ok=True)
        os.makedirs(os.path.join(self.test_dir, ".vscode"), exist_ok=True)
        
        # Create test files
        files = [
            "test.txt",
            "docs/document.md",
            ".git/HEAD",
            ".git/config",
            ".vscode/settings.json",
            ".DS_Store"  # Hidden file on macOS
        ]
        
        for rel_path in files:
            file_path = os.path.join(self.test_dir, rel_path)
            dir_path = os.path.dirname(file_path)
            if not os.path.exists(dir_path):
                os.makedirs(dir_path, exist_ok=True)
            with open(file_path, "w") as f:
                f.write(f"Content of {rel_path}")
    
    def test_should_ignore(self):
        """Test should_ignore function"""
        # Test with various patterns
        ignore_patterns = [".*", "*.pyc", "__pycache__"]
        
        # These should be ignored
        self.assertTrue(should_ignore(".git", ignore_patterns))
        self.assertTrue(should_ignore(".vscode", ignore_patterns))
        self.assertTrue(should_ignore(".DS_Store", ignore_patterns))
        self.assertTrue(should_ignore("test.pyc", ignore_patterns))
        self.assertTrue(should_ignore("__pycache__", ignore_patterns))
        
        # These should not be ignored
        self.assertFalse(should_ignore("test.txt", ignore_patterns))
        self.assertFalse(should_ignore("docs", ignore_patterns))
    
    def test_update_index(self):
        """Test index updating"""
        # Use a more relaxed ignore pattern to include our test files
        ignore_patterns = []  # Don't ignore any files for this test
        
        # Update the index with the specific DB path
        update_index(self.test_dir, ignore_patterns, self.db_path)
        
        # Verify results by querying the database with a fresh connection
        db = DatabaseManager(self.db_path)
        try:
            files = db.get_files_by_criteria(directory=self.test_dir)
            
            # We should have at least 2 files that aren't ignored
            self.assertGreaterEqual(len(files), 2)
            
            # Get absolute paths for comparison
            file_paths = [f["absolute_path"] for f in files]
            
            # Verify specific files
            self.assertIn(os.path.join(self.test_dir, "test.txt"), file_paths)
            self.assertIn(os.path.join(self.test_dir, "docs/document.md"), file_paths)
            
            # Check for hidden files - if we've removed the ".*" pattern, they should be included
            hidden_path = os.path.join(self.test_dir, ".git/HEAD")
            if ".*" not in ignore_patterns:
                self.assertIn(hidden_path, file_paths)
            else:
                self.assertNotIn(hidden_path, file_paths)
        finally:
            # Make sure to close the database connection
            db.close()

if __name__ == "__main__":
    unittest.main()

================
File: tests/test_search_pipeline.py
================
# tests/test_search_pipeline.py
import os
import sys
import unittest
import tempfile
import shutil
from unittest.mock import MagicMock, patch

# Add src directory to path to import lfind
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.lfind.db_manager import DatabaseManager
from src.lfind.search_pipeline import SearchPipeline
from src.lfind.embed_manager import EmbedManager
from src.lfind.embedding.service import EmbeddingService
from src.lfind.llm_service import LLMService

class TestSearchPipeline(unittest.TestCase):
    def setUp(self):
        """Set up test environment"""
        # Create a temporary directory for test files
        self.test_dir = tempfile.mkdtemp()
        
        # Create a temporary database file
        self.db_path = os.path.join(self.test_dir, "test_metadata.db")
        self.db = DatabaseManager(self.db_path)
        
        # Mock the embedding service and LLM service
        self.embedding_service = MagicMock(spec=EmbeddingService)
        self.llm_service = MagicMock(spec=LLMService)
        
        # Create basic configuration
        self.config = {
            "max_entries": 100,
            "ignore_patterns": [".*"],
            "include_empty_dirs": False,
            "cache_dir": self.test_dir,
            "cache_validity_days": 7
        }
        
        # Create the search pipeline
        self.search_pipeline = SearchPipeline(
            self.db,
            self.embedding_service,
            self.llm_service,
            self.config
        )
        
        # Create some test files and add them to the database
        self.create_test_files()
    
    def tearDown(self):
        """Clean up test environment"""
        self.db.close()
        shutil.rmtree(self.test_dir)
    
    def create_test_files(self):
        """Create test files in the temporary directory and add to DB"""
        # Create directory structure
        os.makedirs(os.path.join(self.test_dir, "docs"), exist_ok=True)
        os.makedirs(os.path.join(self.test_dir, "src"), exist_ok=True)
        
        # Define test files
        files = [
            ("test.txt", "text file", 1000),
            ("docs/document.md", "markdown document", 2000),
            ("src/code.py", "python code", 3000),
            ("src/script.js", "javascript code", 4000)
        ]
        
        # Create files and add to database
        for rel_path, content_type, size in files:
            file_path = os.path.join(self.test_dir, rel_path)
            
            # Create directory if needed
            dir_path = os.path.dirname(file_path)
            os.makedirs(dir_path, exist_ok=True)
            
            # Create file with some content
            with open(file_path, "w") as f:
                f.write(f"This is a {content_type} for testing.")
            
            # Add file to database with extension explicitly extracted
            file_name = os.path.basename(file_path)
            _, extension = os.path.splitext(file_name)
            
            file_data = {
                "name": file_name,
                "absolute_path": file_path,
                "type": "file",
                "size": size,
                "extension": extension  # Make sure extension is set correctly
            }
            self.db.touch_file(file_data)
    
    def test_filter_by_extension(self):
        """Test filtering files by extension"""
        # Test with Python files - be explicit about the extension format
        python_files = self.search_pipeline.filter_by_extension([".py"], self.test_dir)
        self.assertEqual(len(python_files), 1)
        self.assertEqual(os.path.basename(python_files[0]["absolute_path"]), "code.py")
        
        # Test with multiple extensions
        script_files = self.search_pipeline.filter_by_extension([".py", ".js"], self.test_dir)
        self.assertEqual(len(script_files), 2)
        
        # Test with text files
        text_files = self.search_pipeline.filter_by_extension([".txt"], self.test_dir)
        self.assertEqual(len(text_files), 1)
    
    @patch("src.lfind.search_pipeline.SearchPipeline.search_llm")
    def test_multi_search_with_llm(self, mock_search_llm):
        """Test multi-stage search with LLM"""
        # Configure mock to return an actual file that exists
        mock_search_llm.return_value = [
            {"name": "code.py", "absolute_path": os.path.join(self.test_dir, "src/code.py"), "id": 3, "type": "file"}
        ]
        
        # Perform search
        results = self.search_pipeline.multi_search(
            query="find python code",
            directory=self.test_dir,
            extensions=[".py", ".js", ".txt"],
            use_semantic=False,
            use_llm=True,
            top_k=5,
            filter_criteria={}  # Make sure to pass an empty dict if needed
        )
        
        # Check results
        self.assertEqual(len(results), 1)
        self.assertEqual(os.path.basename(results[0]["absolute_path"]), "code.py")
        
        # Verify LLM search was called
        mock_search_llm.assert_called_once()

if __name__ == "__main__":
    unittest.main()
